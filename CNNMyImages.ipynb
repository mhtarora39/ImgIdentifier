{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from resizeimage import resizeimage\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Globel Variables \n",
    "learning_rate=0.01\n",
    "batch_size =16\n",
    "epochs = 100\n",
    "num_classes = 3\n",
    "training_iters=1\n",
    "weights = {\n",
    "        'wc1': tf.get_variable('W0', shape=(3,3,1,32),initializer=tf.contrib.layers.xavier_initializer()),#Xavier Glorot and Yoshua Bengio (2010)\n",
    "        'wc2': tf.get_variable('W1', shape=(3,3,32,64), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "        'wc3': tf.get_variable('W2', shape=(3,3,64,128), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "        'wd1': tf.get_variable('W3', shape=(32*32*128,128), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "        'out': tf.get_variable('W4', shape=(128,num_classes), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    }\n",
    "biases = {\n",
    "        'bc1': tf.get_variable('B0', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        'bc2': tf.get_variable('B1', shape=(64), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        'bc3': tf.get_variable('B2', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        'bd1': tf.get_variable('B3', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        'out': tf.get_variable('B4', shape=(num_classes), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "   }\n",
    "parameters = {**weights,**biases}\n",
    "x =tf.placeholder('float32',[None,256,256,1],name='input')\n",
    "y =tf.placeholder('float32',[None,num_classes],name='output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters = {**weights,**biases}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_BW(Inputpath,size=[256,256,1],DircName='Out'):    \n",
    "    outputPath=[]\n",
    "    for pat in Inputpath:\n",
    "        if not os.path.exists(pat+'/'+DircName):\n",
    "               os.mkdir(pat+'/'+DircName, 777 ) \n",
    "        outputPath.append(pat+'/'+DircName)  \n",
    "    for i,path in  enumerate(Inputpath):\n",
    "        listof = os.listdir(path)\n",
    "        for img in listof:\n",
    "                 if(img!=DircName): \n",
    "                     im = Image.open(path+'/'+img).convert('L')\n",
    "                     cover =  resizeimage.resize_contain(im,size)\n",
    "                     cover =cover.convert('L')\n",
    "                     Image_array = np.array([np.array(cover)])\n",
    "                     cover.save(outputPath[i]+'/'+img,'PNG')\n",
    "    return outputPath\n",
    "\n",
    "def DoAgument(Inputpath,DircName='Agu'):\n",
    "    outputPath =[]\n",
    "    outputPaths =[]\n",
    "    count = 1\n",
    "    for pat in Inputpath:\n",
    "        outputPaths.append(pat+'/'+DircName)\n",
    "        if not os.path.exists(pat+'/'+DircName):\n",
    "               os.mkdir(pat+'/'+DircName, 777 ) \n",
    "    gen = ImageDataGenerator(rotation_range=10, width_shift_range=0.05,height_shift_range=0.05,horizontal_flip=False)\n",
    "    for k,input_path in enumerate(Inputpath):\n",
    "        listof  =os.listdir(input_path)\n",
    "        outputPath.append(input_path+'/'+DircName+'/random{}.png')\n",
    "        for j in range(len(listof)):\n",
    "            if(listof[j]!=DircName):\n",
    "                # load image to array\n",
    "                image = np.array([np.array(Image.open(input_path+'/'+listof[j]))])  \n",
    "                # reshape to array rank 4\n",
    "                image = image.reshape(1,256,256,1)\n",
    "                # let's create infinite flow of images\n",
    "                images_flow = gen.flow(image, batch_size=1)\n",
    "                for i, new_images in enumerate(images_flow):\n",
    "                    # we access only first image because of batch_size=1\n",
    "                   \n",
    "                    img = array_to_img(new_images[0]);\n",
    "                    img.save(outputPath[k].format(j*10+i+1),\"PNG\")\n",
    "                   # plt.imsave(arr=new_images[0],fname=output_path.format(j*10+i+1))\n",
    "                    # new_image = Image.fromarray(new_images[0].reshape(256,256))\n",
    "                    #plt.imsave(arr=new_images[0].reshape(256,256),fname=output_path.format(j*10+i+1),cmap='gray')\n",
    "                    if i >= count:\n",
    "                        break   \n",
    "                        \n",
    "    return outputPaths\n",
    "                        \n",
    "def GetLabeledData(ListOfDir):\n",
    "    count =[]\n",
    "    totel =0\n",
    "    NoteData=[]\n",
    "    for path in ListOfDir:\n",
    "        listof = os.listdir(path)\n",
    "        totel = totel+len(listof)\n",
    "        count.append(totel)     \n",
    "    print(totel)    \n",
    "    Labels = np.zeros((totel,len(ListOfDir)))\n",
    "    Data = np.zeros((totel,256*256))\n",
    "    Last_index=0 \n",
    "    for i,index in enumerate(count):\n",
    "        print(Last_index,index)\n",
    "        Labels[Last_index:index-1,i] = 1\n",
    "        Labels[index-1,i] =1\n",
    "        listof = os.listdir(ListOfDir[i])\n",
    "        Data[Last_index:index,:] =  np.array([np.array(Image.open(ListOfDir[i]+'/'+lm)).flatten() for lm in listof])\n",
    "        Last_index =index\n",
    "    return Data,Labels\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunPipeline(InputPath):\n",
    "    paths = make_BW(InputPath)\n",
    "    newPaths = DoAgument(paths)\n",
    "    Images,Labels =GetLabeledData(newPaths)\n",
    "    return Images,Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestDataLabels(Data,Labels,Index):\n",
    "    f,ax = plt.subplots(1,len(Index),sharey=True,figsize=(50,50))\n",
    "    for i,ind in enumerate(Index):\n",
    "        ax[i].imshow(Data[ind].reshape(256,256),cmap='gray')\n",
    "        ax[i].set_title(Labels[ind],fontsize=50)     \n",
    "    plt.show();\n",
    "    \n",
    "def TestTrainPreprocess(Data,Labels,Test_size=0.3):\n",
    "    Data = Data.astype('float32')\n",
    "    Data = Data/ 255.\n",
    "    Data = Data.reshape(-1,256,256,1)\n",
    "    return train_test_split(Data,Labels,test_size=Test_size,random_state=341,shuffle=True)\n",
    "   \n",
    "def CreateTensorGraph():\n",
    "    tf.reset_default_graph() \n",
    "    weights = {\n",
    "        'wc1': tf.get_variable('W0', shape=(3,3,1,32),initializer=tf.contrib.layers.xavier_initializer()),#Xavier Glorot and Yoshua Bengio (2010)\n",
    "        'wc2': tf.get_variable('W1', shape=(3,3,32,64), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "        'wc3': tf.get_variable('W2', shape=(3,3,64,128), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "        'wd1': tf.get_variable('W3', shape=(32*32*128,128), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "        'out': tf.get_variable('W4', shape=(128,num_classes), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    }\n",
    "    biases = {\n",
    "        'bc1': tf.get_variable('B0', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        'bc2': tf.get_variable('B1', shape=(64), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        'bc3': tf.get_variable('B2', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        'bd1': tf.get_variable('B3', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        'out': tf.get_variable('B4', shape=(num_classes), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "   }\n",
    "    x =tf.placeholder('float32',[None,256,256,1],name='input')\n",
    "    y =tf.placeholder('float32',[None,num_classes],name='output')\n",
    "    parameters = {**weights,**biases}\n",
    "    return weights,biases,x,y,parameters\n",
    "\n",
    "\n",
    "    \n",
    "def conv2d(x,w,b,strides=1):\n",
    "    x = tf.nn.conv2d(x,w,strides=[1,strides,strides,1],padding='SAME')\n",
    "    x = tf.nn.bias_add(x,b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x,k=2):\n",
    "    return tf.nn.max_pool(x,ksize=[1,k,k,1],strides=[1,k,k,1],padding='SAME')\n",
    "\n",
    "\n",
    "\n",
    "def conv_net(x, weights, biases):\n",
    "    ###input \n",
    "    conv1 = conv2d(x,weights['wc1'],biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1,k=2)\n",
    "    dropl =  tf.nn.dropout(conv1,0.75) \n",
    "    conv2 = conv2d(dropl,weights['wc2'],biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2,k=2)\n",
    "    drop2 =  tf.nn.dropout(conv2,0.8)\n",
    "    conv3 = conv2d(drop2,weights['wc3'],biases['bc3'])\n",
    "    conv3 = maxpool2d(conv3,k=2)\n",
    "    drop3 =  tf.nn.dropout(conv3,0.8)\n",
    "    fc1   = tf.reshape(drop3,[-1,weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1   = tf.add(tf.matmul(fc1,weights['wd1']),biases['bd1'])\n",
    "    fc1   = tf.nn.relu(fc1)\n",
    "    drop4 =  tf.nn.dropout(fc1 , 0.9) \n",
    "    out   = tf.add(tf.matmul(drop4,weights['out']),biases['out'])\n",
    "    return out\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoSomeCoolStuff(train_X,train_y,test_X,test_y,path=\"./tmp/model3.ckpt\"):\n",
    "   # weights,biases,x,y,parameters =CreateTensorGraph()\n",
    "    pred = conv_net(x, weights, biases)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    init = tf.initialize_all_variables()\n",
    "    tf.local_variables_initializer()\n",
    "    saver = tf.train.Saver(parameters)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init) \n",
    "        train_loss = []\n",
    "        test_loss = []\n",
    "        train_accuracy = []\n",
    "        test_accuracy = []\n",
    "        for i in range(training_iters):\n",
    "            for batch in range(len(train_X)//batch_size):\n",
    "                batch_x = train_X[batch*batch_size:min((batch+1)*batch_size,len(train_X))]\n",
    "                batch_y = train_y[batch*batch_size:min((batch+1)*batch_size,len(train_y))]    \n",
    "                opt = sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "                loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y})\n",
    "            print(\"Iter \" + str(i) + \", Loss= \" + \\\n",
    "                          \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.5f}\".format(acc))\n",
    "            print(\"Optimization Finished!\")\n",
    "\n",
    "            test_acc,valid_loss = sess.run([accuracy,cost], feed_dict={x: test_X,y: test_y})\n",
    "            train_loss.append(loss)\n",
    "            test_loss.append(valid_loss)\n",
    "            train_accuracy.append(acc)\n",
    "            test_accuracy.append(test_acc)\n",
    "            print(\"Testing Accuracy:\",\"{:.5f}\".format(test_acc))\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(sess, \"./tmp/model3.ckpt\")\n",
    "        ##test_acc,valid_loss = sess.run([accuracy,cost], feed_dict={x:DatasetTest,y: LabelsTest})    \n",
    "        ##print(test_acc,valid_loss)    \n",
    "        \n",
    "    return saver,save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run():\n",
    "    path12 = \"Add Path",
    "    path22 = \"Add Path"\n",
    "    path13 = \"Add Path"\n",
    "    listofInput = [path12,path22,path13]\n",
    "    Data,Labels = RunPipeline(listofInput)\n",
    "    TestDataLabels(Data,Labels,[60,120,180,240])\n",
    "    X_train, X_test, y_train, y_test=TestTrainPreprocess(Data,Labels)\n",
    "    path = DoSomeCoolStuff(X_train,y_train,X_test,y_test)\n",
    "    return path\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver,save_path= Run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path12Test = \"C:/Users/ma185280/Desktop/dataa/Test_100\"\n",
    "path22Test = \"C:/Users/ma185280/Desktop/dataa/Test_500\"\n",
    "path13Test = 'C:/Users/ma185280/Desktop/dataa/OthersTest'\n",
    "InputPath = [path12Test,path22Test,path13Test]\n",
    "DataTest,LabelsTest = GetLabeledData(InputPath)\n",
    "DataTest = DataTest.astype('float32')\n",
    "DataTest = DataTest/ 255.\n",
    "DataTest = DataTest.reshape(-1,256,256,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess1:\n",
    "    saver.restore(sess1, \"./tmp/model3.ckpt\")\n",
    "    pred = conv_net(x, weights, biases)\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    init = tf.global_variables_initializer()\n",
    "    print(\"Model restored.\")\n",
    "    print('Accuracy :',sess1.run(accuracy,feed_dict={x:DataTest,y:LabelsTest}))\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
